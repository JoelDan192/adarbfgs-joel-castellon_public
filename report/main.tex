\documentclass[12pt,conference,compsocconf]{IEEEtran}
\usepackage{float}
\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment
\usepackage{url}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\usepackage{times}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{subfig}
\usepackage{amsthm}
\usetikzlibrary{arrows,shapes}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand{\E}{{\rm I\kern-.3em E}}

\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\R}{\mathbb{R}}


\newtheorem{innercustomgeneric}{\customgenericname}
\providecommand{\customgenericname}{}
\newcommand{\newcustomtheorem}[2]{%
  \newenvironment{#1}[1]
  {%
   \renewcommand\customgenericname{#2}%
   \renewcommand\theinnercustomgeneric{##1}%
   \innercustomgeneric
  }
  {\endinnercustomgeneric}
}

\newcustomtheorem{customthm}{Theorem}
\newcustomtheorem{customlemma}{Lemma}
\newcustomtheorem{customcorollary}{Corollary}
\newcustomtheorem{customconjecture}{Conjecture}
\newcustomtheorem{customdefinition}{Definition}
\usepackage{setspace}
\setstretch{1.05}

\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{tikz}

\newcommand\tikzmark[1]{%
  \tikz[remember picture,overlay]\node[inner sep=2pt] (#1) {};}
\newcommand\DrawBox[3][]{%
  \tikz[remember picture,overlay]\draw[#1] ([xshift=-3.5em,yshift=7pt]#2.north west) rectangle (#3.south east);}

\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}

\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}

\begin{document}
\renewcommand{\arraystretch}{1.2}

\title{Complexity analysis for AdaRBFGS: a primitive for methods between first and second order}

\author{
  Joel Castellon\\
  \textit{Supervisor:} Dr. Sebastian Stich and Prof. Martin Jaggi\\
  \vspace{4mm}
  \textit{Machine Learning and Optimization Laboratory, IC School, EPFL}\\
}


\maketitle
\begin{abstract}
There is a fundamental trade-off in first and second order methods between complexity of the updates (e.g. time and memory) and surface information an algorithm can incorporate per iteration. AdaRBFGS, a sub-routine for such methods, iteratively approximates matrices (such as the Hessian) and incorporates a dimension reducing sketch while adaptively accelerating its convergence. Our work aims to shed some light on the convergence behavior of AdaRBFGS which was proposed as an heuristic in \cite{Gower1}.
\end{abstract}

\section{Introduction}
Matrix inversion algorithms are a fundamental sub-routine in optimization methods. Such sub-routines play an important role in the trade-off between incorporating surface information and time/memory efficiency. Which is particularly important in second order methods. In this work we consider adaRBFGS as a primitive for minimizing $f(x)$ that is smooth and convex (additional assumptions such as separability permit further specializations as in \cite{Gower2}). Perhaps, the most well-known example is Newton's method with updates of the form $x_{k+1}=x_k-\alpha_kH_k^{-1}\nabla f(x_k)$ (where $\alpha_k$ is the step size, $x_k$ newton's method iterates and $H_k$ the Hessian of $f$ at iteration $k$). In such updates we are required to calculate the inverse of the Hessian (at each step) in order to incorporate second-order curvature information. While Newton's method has an excellent convergence rate (e.g. quadratic), the computational burden of calculating the inverse Hessian or even storing it. This amounts to $O(n^2)$ for storage and $O(n^3)$ flops to calculate such update, with no prior structure or information about the Hessian. Making it in-adequate for many applications. On the other hand, a gradient descent update has the form $x_{k+1}=x_k-\alpha_k\nabla f(x_k)$ with $O(n)$ per iteration complexity both in time and memory. Then, the question is whether we can get some of the best of both worlds.\\
The motivation for this work comes from studying a primitive proposed by Gower et al. in \cite{Gower1}. In Gower's work, the adaRBFGS method is proposed in an heuristic manner and stems from a more general setting that has solid theoretical ground. The authors compare, empirically, adaRBFGS against other baselines for inverting positive definite matrices and adaRBFGS outperforms these by far. Yet, there is still little understanding of the convergence properties of the rate for adaRBFGS as well as other more specific properties (such as dependence on the dimension of the sketch matrix, see Subsection \ref{sect:sketchdim}). Such fine grained analysis of the rate for adaRBFGS is the subject of this work and also a suggested direction for further reasearch by Gower et al. in \cite{Gower1}.

\section{Outline}
We start this report by describing the general framework that is at the origin of adaRBFGS in Section \ref{sect:rbfgs} as background. The proposed method by Gower et al. is interesting, first, because of the generalization (c.f. Subsection \ref{sect:framework}) and the connexion this framework makes with previously well known family of methods such as quasi-newton and simultaneous Kaczmarz just to name a few. In Section \ref{sect:propertiesRBFGS} we mention some results and properties of adaRBFGS that will be useful for our complexity analysis. We will, then, show our results with respect to the rate complexity (Section \ref{sect:results}) of the rate via numerical experiments and theoretical justification. Finally, we make suggestions for future research and open problems we encountered in Section \ref{sect:conclusion}.


\section{Notation}\label{sect:notation}
We briefly outline some notation and assumptions that will be used in this report. We follow closely the notation established by Gower et al. in \cite{Gower1} to make the parallel between such work and ours easier to follow. First, the matrix that we aim to invert is $A \in \R^{n\times n}$ which is assumed to be symmetric and positive definite (extensions for non-symmetric and non-psd can be found in \cite{Gower1,Gower4}). Then, $X_k \in \R^{n\times n}$ are the sequence of iterates that aim to approximate $A$. We often refer to the corresponding Cholesky factors $L_k \in \R^{n\times n}$ (where $X_k=L_kL_k^T$, such decomposition stems from \eqref{eq:updaterule} when we discuss sampling (as in Subsection \ref{sect:convsampling}). Next, $S \in \R^{n\times q}$ is the sketch matrix to be sampled at each iteration with support denoted by $\mathcal{S} \in \R^{n \times n}$ (details on such distribution are given in Subsection \ref{sect:completediscrete}). Also, let \eqref{eq:norm} be the weighted Frobenius norm.
\begin{align} \label{eq:norm}
	\norm{X}_{F(W^{-1})}^2 &=Tr(X^TW^{-1}XW^{-1})\\ \nonumber
    &=\norm{W^{-1/2}XW^{-1/2}}_F^2\\ \nonumber
\end{align}
Where $W \in \R^{n \times n}$ is a parameter of the algorithm as presented in \cite{Gower1} (this is set to $A^{-1}$ for adaRBFGS). And, $\norm{.}_F$ is the standard Frobenius norm.
\section{RBFGS}\label{sect:rbfgs}
AdaRBFGS emerges from a framework proposed by Gower et al. in \cite{Gower1,Gower2} to which we refer as \textit{Stochastic Iterative Matrix Inversion} (see Algorithm \ref{algo:RBFGS}). The idea is to generalize quasi-newton (qN) updates, which are the primitives (matrix updates for approximating the inverse Hessian) in qN-methods. For example, take the following unconstrained optimization problem (with $f$ smooth and convex).
$\text{min}_{x \in \R^n} f(x)$. The quasi-newton iteration being, 
\begin{align}\label{eq:qnmethod}
x_{k+1}=x_k-\alpha_kB_k^{-1}\nabla f(x_k)
\end{align}
Now, any quasi-newton matrix $B_k$ (approximation of the Hessian) must satisfy the following constraint.

\begin{align}\label{eq:secant}
B_{k+1}(x_{k+1}-x_k)=\nabla f(x_k)-\nabla f(x_k)
\end{align}
This last expression is normally referred as the \textit{secant equations}.
\vspace{-4mm}
\subsection{General setting and geometric interpretation}\label{sect:framework}
The first idea of Gower et al. in \cite{Gower1,Gower2} is to reduce the dimension of the system in \eqref{eq:secant} while maintaining low rank updates to matrices that aim to approximate the inverse Hessian. AdaRBFGS as developed in \cite{Gower1} only considers the following version of the unconstrained optimization problem $f(x)=\frac{1}{2}x^TAx -b^Tx + c$. Furthermore, the secant equations become $AX=I$.  Therefore, we have $A^{-1}$ as our target inverse Hessian, and the sequence of iterates to approximate $A^{-1}$ \footnote{yet the authors of \cite{Gower1} claim it can be adapted to approximate arbitrary matrices} are given by:
\begin{align}\label{eq:sketchandproject}
X_{k+1} = &\text{ argmin}_{X \in \R^{n\times n}} \{\frac{1}{2}\norm{X-X_k}_{F(W^{-1})}^2 \\ \nonumber
&\hspace{18mm}\text{subject to } S^TAX=S^T\} \nonumber
\end{align}
 $W \in \R^{n\times n}$ being a parameter of the algorithm, which is set to $A^{-1}$ for adaRBFGS. Where $S \in \R^{n\times q}$ and $q \ll n$. We call $S$ the sketch matrix that will reduce the dimension of the system of equations. For example, in the extreme case one can take $S=e_i$. For such choice we have to solve a system with $1$ row and $n$ columns (e.g. $e_i^TAX=e_i^T$). Another basic case is when we set $S=I$, this yields the original secant equations for our problem (e.g. $AX=I$). In its general form, $S$ can be considered a random matrix, we draw $S$ either from a fixed or varying distribution (see Subsection \ref{sect:adadef}). Such reduction in complexity for the constraints is, of course, at the expense of introducing many spurious solutions which is why we aim to take small rank updates. Gower et al. call this first formulation for the $X_k$ updates the \textit{sketch-and-project} point of view. Note that the sketch-and-project point of view is a convex quadratic problem with affine constraints. Therefore, strong duality holds and the following dual update rule can be derived (see \cite{Gower1})
 \begin{align}\label{eq:constrainandapprox}
 X_{k+1} = &\text{ argmin}_{X,Y} \{\frac{1}{2}\norm{X-A^{-1}}_{F(W^{-1})}^2\\ \nonumber
 &\hspace{13mm}\text{subject to } X=X_k + WA^TSY^T\}
 \end{align}
 
 This last expression is referred to as the \textit{constrain-and-approximate} point of view. There are two interesting points about this dual formulation. First, the latter type of update corresponds to the \textit{Approximate Inverse Preconditioning (AIP)} family of methods, while the former kind of update corresponds to qN updates. Then, this framework shows a previously unknown connection between qN and AIP methods. The second important point about the constrain-and-approximate rule is a geometrical interpretation which gives a more intuitive understanding of what the method does. What the constrain-and-approximate update rule says is that at each iterate $X_k$ we draw a random hyperplane crossing such point (over the distribution of $S$). Next, take the orthogonal projection of the target $A^{-1}$ in such random hyperplane as the iterate. Hence, the algorithm consists of drawing random hyperplanes crossing the iterates and projecting the objective which is $A^{-1}$.

\begin{algorithm}
\caption{Stochastic Iterative Matrix Inversion}\label{algo:RBFGS}
\begin{algorithmic}[1]
\Statex
\INPUT\tikzmark{a}  
\Statex $A \in \R^{n \times n}$ \Comment a symmetric matrix to be inverted
\Statex $\mathcal{D}$  \Comment distribution over random matrices
\Statex $W \in \R^{n \times n}$ \Comment symmetric pos. def. $W=A^{-1}$ for RBFGS
\Statex $X_0 \in \R^{n\times n}$  \Comment symmetric initial iterate\tikzmark{b}

\For{k=0,1,2\ldots}
\\
	\hspace{5mm} Sample an independent copy $S\sim \mathcal{D}$\\
    \hspace{5mm} Compute $\Lambda=S(S^TAWAS)^{-1}S^T$\\ \hspace{5mm} $\Theta=\Lambda AW$, $M_k=X_kA-I$
    \vspace{-2mm}
    \begin{align}\nonumber
    	X_{k+1}=X_k&-M_k\Theta-(M_k\Theta)^T\\ \nonumber
        &+\Theta^T(AX_kA-A)\Theta \nonumber
    \end{align}
\EndFor
\OUTPUT\tikzmark{a}
last iterate $X_k$
\tikzmark{b}
\end{algorithmic}
\end{algorithm}

\subsection{RBFGS}\label{sect:rbfgsbackground}
In the framework described above there are three main parameters (see Algorithm \ref{algo:RBFGS}). Namely, the distribution of random matrices $\mathcal{D}$ where $S\sim \mathcal{D}$, the weights matrix $W$ and assumptions on $A$ (e.g. symmetric or  positive definite). It turns out that most well-known quasi newton methods can be obtained by varying these these three parameters (see \cite{Gower1} for a full description). The most relevant result, for us, in this category is obtaining the Broyden-Fletcher-Goldfarb-Shanno (BFGS) by setting $W=A^{-1}$, $S$ as a deterministic vector and $A$ semi positive definite. The next step is to let $S$ be a random matrix (of full column rank) that we draw a fixed distribution for which we obtain RBFGS. RBFGS is a randomized block variant of the BFGS method as it can be seen as taking a block of qN-directions per iteration. This can be seen, for example, in the derivation of the qN updates in \cite{Gratton} where $S$ is the scaled direction of the update (e.g. a vector). \footnote{More generally, the authors of \cite{Gratton} set $S$ as a random matrix (this time without a rigorous derivation) and proceed with this setup in the rest of \cite{Gratton}}\\
The main result, that will concern us, regarding the rate of this non-adaptive algorithm (e.g. RBFGS) is Thm. 6.1 in \cite{Gower1} which is the analogous of Thm. \ref{thm:onestep} we outline in this report.

\begin{customthm}{6.2}[Gower et al. 2016]\label{thm:rateRBFGS}
After applying $k$ iterations of Algorithm \ref{algo:RBFGS} (Assuming $A$ and $X_0$ are symmetric) we have:

\begin{align}\label{eq:progressRBFGS}
	\E\big[\norm{X_{k}-A^{-1}}^2_{F(W^{-1})}\big]\leq \rho^k \norm{X_0-A^{-1}}^2_{F(W^{-1})}
\end{align}
where $\rho=1-\lambda_{\text{min}}(W^{1/2}\E\big[Z]W^{1/2})$, \\ 
and $0\leq 1 - \E [q]/n \leq \rho \leq 1$.
\end{customthm}
Where the expectation is taken over the distribution of $S$. Note that in Thm. \ref{thm:rateRBFGS}, the rate depends on the quantity $1-\rho =\lambda_{\text{min}}(W^{1/2}\E\big[Z]W^{1/2})$. Where $W^{1/2}ZW^{1/2}$ is a projection matrix (as shown in Lemma 6.1 in \cite{Gower1}) that projects the iterates $X_k$ into a sketch of the column space of $A$ (e.g. $W^{1/2}A^TS$, see Lemma 6.1 in \cite{Gower1} for details). In fact, the form that $1-\rho$ has will be recurrent in our analysis as there is a adaptive counterpart (in Thm.\ref{thm:onestep}). Furthermore, our results rely on bounds that will be obtained for such adaptive rate (Section \ref{sect:results}) similarly to Thm.\ref{thm:rateRBFGS} above.

\begin{figure}
  \centering
  \includegraphics[height=0.8\columnwidth,width=1.0\columnwidth]{interlacing.eps}
  \vspace{-6mm}
  \caption{Each trajectory corresponds to the eigenvalue of a fixed rank (e.g. smallest, second smallest, \ldots, largest) across iterations. This plot corresponds to the spectrum of a $AX_k \in \R^{300\times300}$ with $A$ being symmetric and positive definite. This experiment puts in evidence the interlacing spectrum property of adaRBFGS (column variant) on which we elaborate in Thm. \ref{thm:interlacing} \label{fig:interlacing}}
\end{figure}

\subsection{Adaptive RBFGS}\label{sect:adadef}
Next, we outline the intuition for adaRBFGS as derived in \cite{Gower1}. First, the main convergence result in \cite{Gower1} is Thm. 6.2 which outlines a rate (see Section \ref{sect:results}) in expected norm of the error. We assume that the sketch matrix sampled at each iteration $S_i$ comes from a bigger matrix (its support) which we call $\mathcal{S}$ (conditions on Section \ref{sect:propertiesRBFGS}). In section 7.1 of \cite{Gower1} the problem of determining optimal sampling probabilities is formulated as a semi definite program. Solving such SDP, however, can be even costlier than inverting $A$ itself as we iterate (support given by experiments by the authors in \cite{Gower1}). Nonetheless, the formula for the optimal probabilities obtained by the SDP (formula (62) in \cite{Gower1}) is insightful in the sense that it suggests that optimal sampling is its dependence on $A^{-1}$. However, since we do not have $A^{-1}$ we can use $X_k$ as a proxy. This is, in fact, the intuition behind the heuristic which states that sampling gets better (this being in close relation to the rate, see Section \ref{sect:results}) as the estimate $X_k$ is closer to the target $A^{-1}$. Now, in order to get a specific form for the support $\mathcal{S}$, an upper bound for the rate is derived in \cite{Gower2}. Such upper bound is optimal by setting $\mathcal{S}=A^{-T}W^{-1/2}=A^{-1/2}$. This means that if $X_k$ approximates $A^{-1}$ then we should sample $S=S_i$ (the sketch) at each iteration from the columns of $L_k$ (e.g. the Cholesky factor of $X_k$). In fact an update rule that uses only these factors was derived in \cite{Gratton} in the context of limited memory preconditioners for BFGS. This latter update form is the one adaRBFGS uses in \cite{Gower1}\\
Next, we present two variants for the varying distribution where $S$ is sampled from (e.g. $\mathcal{D}_k$). A first such distribution is the \textit{adaRBFGS gauss} (baseline results in Fig. \ref{fig:comparison}). In adaRBFGS gauss we set $S_i=X_kG$ where $G \in \R^{n\times q}$ is a random matrix of standard gaussian i.i.d entries.\\
The second variant of adaRBFGS we present is the \textit{adaRBFGS col}. This version consists of sampling $S$ as a column sub-matrix $S_i$ from $L_k=[S_1\ldots S_r]$ (where $L_kL_k^T=X_k$) with sampling probabilities $p_i$ (as defined in Subsection \ref{sect:convsampling}). More formally, $S=L_kI{:C_i}=S_i$ where $C_i$ is the index set corresponding to column sub-matrix $S_i$.

\subsection{Adaptive RBFGS as a primitive}\label{sect:primitive}
The RBFGS update rule (which is the same for adaRBFGS but with varying $\mathcal{D}$, see Section \ref{sect:propertiesRBFGS}) has a well-known form in the BFGS literature, and a consequence of this form is that it can be composed in a larger setting to get methods with well-known properties in the quasi Newton family. A concrete example is the work in \cite{Gower3} where the authors combine RBFGS to approximate sub-sampled Hessians, obtain memory efficient variants (in the spirit of L-BFGS) and even combine it with variance reduction techniques for the gradient \cite{variancereduct}. In the more general setting, adaRBFGS can be used as a primitive for preconditioning. In fact, when the output has the guarantee to be positive definite it can be used in the design of variable metric optimization methods (see \cite{Stich1,Leventhal}).\\

\begin{algorithm}
\caption{AdaRBFGS}\label{algo:adaRBFGS}
\begin{algorithmic}[1]
\Statex
\INPUT\tikzmark{a}  
\Statex $A$ \Comment a symmetric positive definite matrix to be inverted
\Statex $\mathcal{D}$  \Comment distribution over random matrices with $n$ rows
\Statex $L_0 \in \R^{n\times n}$  \Comment pick invertible initial iterate\tikzmark{b}

\For{k=0,1,2\ldots}
\\
	\hspace{5mm} Sample an independent copy $\mathcal{S}\sim \mathcal{D}$\\ \hspace{5mm} \Comment $\mathcal{D}$ may also depend on $k$\\ \hspace{34mm} as in the column variant\\ \nonumber
    \hspace{5mm} Compute $S=L_k\mathcal{S}$\\
    \hspace{5mm} \Comment S is sampled adaptively\\  	\hspace{34mm} as it depends on $k$\\
    \hspace{5mm} Compute $R_k
    =(\mathcal{S}^TA\mathcal{S})^{-1/2}$\\
    
 	\begin{align}\nonumber 			\hspace{-10mm} L_{k+1}=L_k&+SR_k((\mathcal{S}^T\mathcal{S})^{-1/2}\mathcal{S}^T\\ \nonumber
    &-R_k^TS^TAL_k)\nonumber
 	\end{align} \Comment Update the factor
\EndFor
\OUTPUT\tikzmark{a}
$X_k=L_kL_k^T$
\tikzmark{b}
\end{algorithmic}
\end{algorithm}



\vspace{-8mm}
\subsection{Numerical experiments}
Next we present results that we reproduced from \cite{Gower1} by re-running one of the main experiments of such work. The difference is, however, that we changed the implementation provided in \cite{Gowercode} to perform the sampling for the column variant of adaRBFGS as stated in \cite{Gower1}. The original implementation performed uniform sampling with respect to the column index, while we now perform \textit{convenient sampling} as in \eqref{eq:convsampingtr} and \eqref{eq:convsamplingmax}. The results are qualitatively the same as in \cite{Gower1}. We plot the error (e.g. distance to $A^{-1}$) against the baseline methods as observed in Fig. \ref{fig:comparison}. Observe that adaRBFGS outperforms largely other well established methods. The experiments in Fig. \ref{fig:comparison} were performed in a $1000\times1000$ symetric positive semi definite matrix. The two methods we use as benchmark are Minimal residual (a method with global convergence) and Newton-Schulz which is basically Newton's method for calculating matrix inverse. As rates for adaRBFGS are the main subject of study for this project we have extended the implementation \footnote{code available at https://github.com/epfml/adarbfgs-joel-castellon} to track the rate, bounds we propose and related metrics.
\begin{figure}
  \centering
  \includegraphics[height=0.7\columnwidth,width=1.1\columnwidth]{benchmark.eps}
  
  \vspace{-2mm}
  \caption{Empirical comparison of Ada-RBFGS variants against two standard methods for matrix inversion (Minimal residual and Newton-Schulz) for a $1000\times1000$ symmetric matrix.\label{fig:comparison}}
\end{figure}

The Matrix used in the experiments is $A=BB^T$ where $B \in \R^{n\times n}$ is obtained by sampling standard Gaussian i.i.d entries. Typical condition numbers for such matrices are in the order of $1e^{10}$. Smaller matrices (e.g. 100,500,700 dimensions) had, correspondingly, condition numbers that were orders of magnitude smaller. We observed that adaRBFGS starts outperforming Minimal Residual and Newton Schulz for larger matrices. For matrices beyond 1000 dimensions MR and Newton Schulz would not even converge.

\section{Properties of RBFGS}\label{sect:propertiesRBFGS}
In this section we outline the main properties that will be useful for our analysis in Section \ref{sect:results} . A part from the main results regarding the rate, we outline some more fundamental properties such as the update rule for the $X_k$. In fact, iterates in the form of \eqref{eq:updaterule} is the reason why adaRBFGS can be used as part of unconstrained convex function minimization methods and known results in BFGS literature can be re-derived for this setting (as outlined in Subsection \ref{sect:primitive}). So, even if the expression for the update below does not seem to be very insightful, it is at the core of the properties we mention here.

\subsection{The update rule}
The update expression can be obtained from the constrain-and-approximate point of view (e.g. Subsection \ref{sect:framework}) by replacing the constraints in the objective and enforcing symmetry with an additional constraint (detailed derivation in \cite{Gower1}). We then obtain:
\begin{align}\label{eq:updaterule}
	X_{k+1}&= S(S^TAS)^{-1}S^T\\ \nonumber
	    &+ (I-S(S^TAS)^{-1}S^TA)X_k(I-AS(S^TAS)^{-1}S^T) \nonumber
\end{align}

One can see from equation \eqref{eq:updaterule} that the update preservers symmetry and positive definitiveness (factorize $X_k$ and its left/right factors are symmetric, see \cite{Gower1} for a full proof). These two latter properties are important since we guarantee all iterates are non-singular, and symmetry is required in the results we use in Subsections \ref{sect:convergence} and \ref{sect:sketchdim} (analysis of convergence and dependence of sketch dimension). Furthermore, this form also makes possible the Cholesky factorization and thus factored update derived in \cite{Gratton}.

\subsection{One-step progress and complete discrete sampling}\label{sect:completediscrete}
The starting point for an analysis of convergence is Thm. 6.2 in \cite{Gower1} (for which the one-step version is Thm.\ref{thm:onestep}). Then, an important question is: what conditions should a sketch matrix satisfy in order to get a result like Thm. 6.2 in \cite{Gower1}. Actually, in order to have such a result the adaptive setting, the authors give a characterization for the support $\mathcal{S}$ where the $S_i$ (sketch matrices) are sampled from. Let $S=S_i\in \R^{n\times q_i}$ with probability $p_i>0$ for $i \in [r]$ where $S_i$ has full column rank. And $S$ is defined as a complete discrete sampling when $\mathcal{S}=[S_1\ldots S_r]\in \R^{n\times n}$ has full row rank.\\
Now, under the condition that the matrix we sample at each iteration is a complete discrete sampling we have:
\begin{customthm}{9.1}[Gower et al. 2016]\label{thm:onestep}
After one step of AdaRBFGS method we have\\

\begin{align}\label{eq:onestepprogress}
	\E\Big[\norm{X_{k+1}-A^{-1}}^2_{F(A)}|X_k\Big] \leq \rho _k\norm{X_k-A^{-1}}^2_{F(A)}
\end{align}
where $1-\rho_k=\lambda_{\text{min}}(A^{-1/2}\E\big[Z|X_k\big]A^{-1/2})$
\end{customthm}

Given that the rate $\rho_k$ does not have an immediate interpretation, the authors in \cite{Gower1} propose a convenient sampling as in \eqref{eq:convsampingtr} which yields the following upper bound $\rho_k \leq 1-\frac{\lambda_{\text{min}}(AX_k)}{Tr(AX_k)}$.\\
Note that such bound depends on the spectrum of $A$ preconditioned by $X_k$. Using the same form, $A$ preconditioned with $X_k$, we obtain in Section \ref{sect:results} a tighter upper bound for the rate and even a lower bound.


\section{Results}\label{sect:results}
\subsection{Interlacing spectrum}\label{sect:interlacing}
As mentioned in Section \ref{sect:propertiesRBFGS}, equation \eqref{eq:updaterule} has some important consequences, among these is the interlacing spectrum (Thm. \ref{thm:interlacing}). Such update rule is a known result in the BFGS literature and can be derived under different circumstances. One such example comes from the equivalent formulation given by conjugate gradient with exact line search (as shown in \cite{Nazareth}). Moreover, a simple corollary of the interlacing spectrum is that the condition number is non-increasing (which we use as motivation in the derivation of our proposed sampling in Thm. \ref{thm:upperbadamax}) is a result independently found by Fletcher in \cite{Fletcher}.
Next, we state the interlacing spectrum theorem as applied to our setting.

\begin{customthm}{2.3}[Interlacing spectrum Gratton 2011]\label{thm:interlacing}
Let $\sigma_1,\ldots\sigma_n$ real positive be the eigenvalues of $AX_{k}$ arranged in non-decreasing order. And, $q$ being the dimension of the sketch matrix $S \in \R^{n\times q}$. Then, the eigenvalues $\mu_1,\ldots,\mu_n$ of $AX_{k+1}$ can be arranged so that\\
\vspace{0mm}
\begin{align}\label{eq:interlacing}
	\left\{
    	\begin{array}{ll}
        	\sigma_j\leq\mu_j\leq\sigma_{j+q}\text{  for } j \in \{1,\ldots,n-q\}\\ 
            \mu_j=1 \text{                           for } j \in\{n-q+1,\ldots,n\}\\
        \end{array}                 
    \right.
\end{align}

\end{customthm}
\vspace{0mm}
It was first observed that adaRBFGS satisfies the interlacing spectrum property by Stich et al. in \cite{Stich1}. The proof is the same as that in \cite{Gratton} where the theorem we present is derived in a slightly different setting, namely, limited-memory preconditioners for least-squares problems. Note, however, that the proof basically relies on the form of the update rule for $X_k$ (details in \cite{Gratton}) which can be factorized in two orthogonal components as presented in the following expression (part of the derivation in \cite{Gratton}).
\vspace{-7mm}
\begin{align}\label{eq:compression}
		A^{1/2}&X_{k+1}A^{1/2}
    	=\\ \nonumber
    	&\begin{bmatrix}
    		A^{1/2}W, & A^{1/2}\underline{W}
    	\end{bmatrix}
        \begin{bmatrix}
			I_q & 0\\
    		0 & V\Lambda V^T
		\end{bmatrix}
    	\begin{bmatrix}
    		W^TA^{1/2}\\
        	\underline{W}^TA^{1/2}
    	  \end{bmatrix} \nonumber
\end{align}
\vspace{-5mm}
\\
Where $W$ and $\underline{W}$ are $A$ orthogonal matrices, and $W=S(S^TAS)^{-1/2}$ (e.g. a term inside the factors that multiply $X_k$ when updating to $X_{k+1}$).
We observe in equation \eqref{eq:compression} that the first component $I_q$ has all 1 eigenvalues. Therefore, at each iteration there are $q$ eigenvalues that are projected to 1. Now, the second component 
\begin{align}\label{eq:compression2}
	V\Lambda V^T=Q^TA^{1/2}X_kA^{1/2}Q 
\end{align}
(where $Q=A^{1/2}\underline{W}$) is a compression of the eigenvalues of the previous iterate (e.g. $AX_{k}$) which yields the interlacing property (see Corollary 4.3.16 in \cite{Horn}).\\
\begin{figure}
  \centering
  \includegraphics[height=0.7\columnwidth,width=1.0\columnwidth]{eigvalcountiter.eps}
  
  \vspace{-2mm}
  \caption{Eigenvalue count for $AX_k$ as a function of iterations for adaRBFGS (column variant). \label{fig:eigcount}}  
  
\end{figure}
We can see interlacing spectrum property (c.f. Thm. \ref{thm:interlacing}) experimentally in Fig. \ref{fig:interlacing}. To confirm this Fig. \ref{fig:interlacing} we clearly see that the eigenvalue trajectories (each trajectory corresponds to a rank) do not cross each other. Furthermore, these are monotonically decreasing for eigenvalues that start above $1$ (in blue) and monotonically increasing for those starting below $1$ (in green). Lastly, note the groups of lines converging (e.g. being projected) at $1$ every few other iterations. Note how this is also described in Thm. \ref{thm:interlacing} and in Proposition 2.8 of \cite{Gratton} (we will use this last proposition in Subsections \ref{sect:convergence}  and \ref{sect:sketchdim} )
\\We also plot (in Fig. \ref{fig:eigcount} ) the count of eigenvalues below, above and at 1 with thresholds defined by a tolerance band of $1e^{-2}$ by default. Note in Fig. \ref{fig:eigcount} both eigenvalues above and below one decrease in a monotone manner while those at 1 increase strictly with the number of iterations which is evidence for the interlacing property. Although the spectrum for our input matrix $A$ always crosses $1$, when this is not the case we can always re-scale $A$ to have spectrum such that $0 < \lambda_{\text{min}}(A) \leq 1$ and $1 \leq \lambda_{\text{max}}(A)$ (see \cite{Stich1,Gratton} without changing the system of equations to solve).\\
One important detail we observe in our experiments is the dependence on the number of iterations until convergence with respect to the initial maximum eigenvalue (e.g. $\lambda_{max}(A)$). To put this in evidence, we show the eigenvalue histogram for adaRBFGS with a tolerance (relative error with respect to the initial error) of $1e^{-2}$ in Fig. \ref{fig:eighist2} and $1e^{-5}$ in Fig. \ref{fig:eighist5}. Observe that for a higher error threshold ($1e^{-2})$ the histogram has a tail to the right side which indicates ill conditioning of $AX_k$. And, as mentioned before, the number of iterations to 'loose' such tail is proportional to $\lambda_{max}(A)$. On the other side, for lower error threshold ($1e^{-5}$) we see that the eigenvalue distribution is centered around $1$ with little variation (e.g. $AX_k \approx I$).
\begin{figure}
  \centering
  \includegraphics[height=0.7\columnwidth,width=1.0\columnwidth]{eigdisttol2.eps}
  
  \vspace{-2mm}
  \caption{Eigenvalue histogram for $AX_k$ after convergence, for the column variant of adaRBFGS, with tolerance $1e{-2}$ for the relative error. \label{fig:eighist2}}  
  
\end{figure}
The authors of adaRBFGS (c.f. \cite{Gower1}) make an intuitive guess on the asymptotic behavior of the spectrum. Namely, the idea of sampling from the Cholesky factor $L_k$ (see Section \ref{sect:adadef}) comes from the expectation of an optimized upper bound on the rate. Namely, for the following upper bound derived by Gower et al in \cite{Gower2}. 
\begin{align} \label{eq:upperbtrada}
	\rho_k \leq 1- \frac{\lambda_{min}(AX_k)}{Tr(AX_k)}
\end{align}
Gower et al. make the conjecture that $ 1- \frac{\lambda_{min}(AX_k)}{Tr(AX_k)}\rightarrow 1 - \frac{1}{n}.$ Hence, with this in mind, the design of adaRBFGS aims to make $AX_k$ well-conditioned in the limit. Yet, Gower et al. only give an intuitive argument (based on Lemma 7.2 in \cite{Gower1} ) on why this should happen and leave  a more detailed analysis for future research.\\
Once the interlacing property has been identified, a first observation is that the rate depends in the condition number (as it often does in BFGS or conjugate gradient methods). In fact, Thm.\ref{thm:interlacing} implies that the matrix $AX_k$ gets better conditioned (e.g. smaller gap between largest and smallest eigenvalues) and we would thus hope to converge to $X_k=A^{-1}$ (as $AX_k\approx I$).
\begin{figure}
  \centering
  \includegraphics[height=0.7\columnwidth,width=1.0\columnwidth]{eigditstol5.eps}
  \caption{Eigenvalue histogram after convergence, for the column variant of adaRBFGS, with tolerance $1e-5$ for the relative error. \label{fig:eighist5}}  
\end{figure}
We conclude this section by exploring the interlacing spectrum property in the adaRBFGS gauss variant in Fig. \ref{fig:eigcountgauss}. We note that we have have a similar behavior as that for the spectrum of the column variant. However, note that those eigenvalues above $1$ get projected to $1$ at a much lower rate as in the column variant case. This experiment gives us some intuition in that $AX_k$ also gets better conditioned for the gaussian variant, which also hints why adaRBFGS in general has great performance against baselines in Fig. \ref{fig:comparison}. However, the results we present in Sections \ref{sect:adaptive rate} and \ref{sect:convsampling} have only been derived for the column variant. Hence, a theoretical analysis for gaussian or other adaptive distributions is left open for extension in future research.
\begin{figure}
  \centering
  \includegraphics[height=0.7\columnwidth,width=1.0\columnwidth]{eigcountiter_gauss.eps}
  \caption{Eigenvalue count as a function of iterations for adaRBFGS for the gaussian variant. \label{fig:eigcountgauss}}  
\end{figure}
\subsection{Lower bound on adaptive rate}\label{sect:adaptive rate}
From Thm. \ref{thm:interlacing}, we have derived the following lower bound
\begin{customthm}{1}[Lower bound on adaptive rate for AdaRBFGS] \label{thm:lowerbada}
Let $\rho_k$ be the adaptive rate as in Thm. \ref{thm:onestep}. Then, we can obtain the following lower bound:
\begin{align}\label{eq:lowerbada}
\rho_k \geq &1-\frac{tr(D^2)}{tr(A^{-1/2}(L_k^T)^{-1} (L_k)^{-1}A^{-1/2})}\\ \nonumber
&=\rho_{lower} \nonumber
\end{align}
Where\\
$D=Diag(\sqrt{p_1}(S_1^TAS_1)^{-1/2},\ldots,\sqrt{p_r}(S_r^TAS_r)^{-1/2})$
\vspace{-5mm}
\begin{proof}
This comes, in fact, from a corollary of the interlace theorem for symmetric matrices as derived in \cite{J Tao,Fang}. Namely, $\lambda_{min}(Q)tr(R)\leq tr(QR)$ for $Q,R$ real symmetric and positive semi-definite. Then, set $Q=A^{-1/2}\E\big[Z|X_k\big]A^{-1/2}=A^{1/2}L_kD^{2}L_k^TA^{1/2}$ (such factorization for $\E\big[Z|X_k\big]$ is detailed in Proposition 7.1 of \cite{Gower1}) and $R=A^{-1/2}(L_k^T)^{-1} (L_k)^{-1}A^{-1/2}$. Note that $tr(R)$ is positive as the factors $L_k$ we maintain are positive definite thanks to the form in equation \eqref{eq:updaterule}.
\end{proof}
\end{customthm}
Compare $\rho_{lower}$ for both adaRBFGS variants in Figures \ref{fig:ratetr} and \ref{fig:ratemax}. Note that this lower bound bound is especially useful for experiments using the sampling strategy proposed in \cite{Gower1} (equation \eqref{eq:convsampingtr}) because the upper bound (e.g. $\rho_{upper}$) is rather loose near convergence (this can be seen in Fig. \ref{fig:ratetr}). Thus $\rho_{upper}$ is not really a good indicator of the potential performance of the true rate (it is almost $2$ orders of magnitude off the true rate). Hence the lower bound, in this case, is a better indicator of the potential performance of the true rate.\\
Finally, we mention that we have also tried another rate lower bound which is a direct consequence of the interlacing theorem (see \cite{J Tao}), namely Schur's theorem $diag(B) \prec \lambda(B)$ where $B$ is the matrix in the definition of $\rho_k$. However, such lower bound did not present a quantitative improvement in our experiments, so we keep the first one.

\begin{figure}[H]
  \centering
  \includegraphics[height=0.7\columnwidth,width=1.1\columnwidth]{ratetr.eps}
  
  \vspace{-2mm}
  \caption{$1-\rho_k=\lambda_{\text{min}}(A^{-1/2}\E\big[Z|X_k\big]A^{-1/2})$ and lower/upper bounds (as in equations \eqref{eq:lowerbada} and \eqref{eq:upperbtrada} respectively). These plots correspond to the column variant of adaRBFGS with $p_i$ as in \eqref{eq:convsampingtr}. \label{fig:ratetr}}
  
\end{figure}

\subsection{A more concise convenient sampling and non-increasing upper bound}\label{sect:convsampling}
The reason why Gower et al. choose the sampling probabilities as 
\begin{align}\label{eq:convsampingtr}
	p_i=\frac{tr(S_i^TAS_i)}{tr(\mathcal{S}^TA\mathcal{S})}
\end{align}
is that such sampling yields an optimal upper bound. Namely, $1-\frac{\lambda_{min}(AX_k)}{tr(AX_k)} \rightarrow 1-\frac{1}{n}$. We remark, however, two potential inefficiencies in this derivation. First, the upper bound $1-\frac{1}{n}$ simply corresponds to $AX_k\approx I$ (see Subsection \ref{sect:interlacing}) thus, we should be able to find $\rho_{upper}$ that converges to a perhaps simpler function of the spectrum of the identity matrix. Second, on the derivation for the rate upper bound in \cite{Gower1} (proof of Thm. 9.1) the sequence of inequalities can be optimized (e.g. we only use the first inequality of such proof) by setting
\begin{align}\label{eq:convsamplingmax}
	p_i=\frac{\lambda_{max}(S_i^TAS_i)}{\lambda_{max}(\mathcal{S}^TA\mathcal{S})}
\end{align}

yields the following result.
\begin{customthm}{2}[A tighter upper bound for adaRBFGS's adaptive rate]\label{thm:upperbadamax} Let $\rho_k$ be the adaptive rate of adaRBFGS as in Thm. \ref{thm:onestep}, we can obtain the following upper bound by setting a convenient sampling of the form $p_i=\frac{\lambda_{max}(S_i^TAS_i)}{\lambda_{max}(\mathcal{S}^TA\mathcal{S})}$:
\begin{align}\label{eq:upperbmaxada}
\rho_{upper}&= 1-\kappa (AX_k)^{-1}\\ \nonumber &=1-\frac{\lambda_{min}(AX_k)}{\lambda_{max}(AX_k)}\\ \nonumber
&\geq \rho_k \nonumber
\end{align}
\begin{proof}
\begin{align}
	\rho_k &\geq \lambda_{\text{min}}(A^{1/2}L_kL_k^TA^{1/2})\lambda_{\text{min}}(D^2)\\ \nonumber
    &=\frac{\lambda_{\text{min}(AX_k)}}{\lambda_{\text{max}}(D^{-2})} \\ \nonumber
    &= \frac{\lambda_{\text{min}(AX_k)}}{\text{max}_i \lambda_{\text{max}}(S_i^TAS_i)/p_i}\\ \nonumber &=\frac{\lambda_{\text{min}(AX_k)}}{\lambda_{\text{max}}(AX_k)} = \kappa(AX_k)^{-1}
\end{align}
The first inequality follows from the factorization for $\E\big[Z|X_k\big]$ as detailed in Proposition 7.1 in \cite{Gower1}. Then, in the first equality we use the fact that $AX_k$ and $A^{1/2}X_kA^{1/2}$ have the same spectrum. The second equality is simply because the maximum eigenvalue of a block diagonal matrix is the maximum among maximum eigenvalues of its block matrices. The third equality follows from our choice of sampling probabilities and the fact that $\mathcal{S}=L_k$ (as explained in Subsection \ref{sect:adadef}).
\end{proof}
\end{customthm}
First note that the sampling we propose in \eqref{eq:convsamplingmax} is in principle more expensive to calculate to the one in \eqref{eq:convsampingtr}. However, \eqref{eq:convsamplingmax} is easier to analyze and we can thus get the results from Section \ref{sect:results}. In addition, we have not noticed much difference with respect to the computational burden for the experiments we performed in Section \ref{sect:results} by adopting \eqref{eq:convsamplingmax} instead of \eqref{eq:convsampingtr}.
Now, one can then outline the advantage of $p_i$ having the form in \eqref{eq:convsamplingmax} by the experiments in Figures \ref{fig:ratemax} and \ref{fig:ratetr}. First, note in those experiments that $\rho_{upper}$ for Gower's $p_i$ yields loose upper bounds. Hence a lower bound is useful to realize the potential value of the true rate as in Subsection \ref{sect:convsampling}. On the other hand, observe (Fig. \ref{fig:ratemax}) that our proposed $\rho_{upper}$ is much tighter (at all times) with respect to the true rate. In addition, note that the sampling we propose can actually make the true rate hit much lower values (in Fig.\ref{fig:ratemax} this corresponds to $1-\rho$ hitting 1) which is not the case for the form of $p_i$ in equation \eqref{eq:convsampingtr} (Fig. \ref{fig:ratetr})\\
Moreover, the theoretical analysis is simpler and we can state the following result as a corollary from Thm. \ref{thm:interlacing}
\begin{align}\label{eq:decrkappa}
	\frac{\lambda_{min}(AX_k)}{\lambda_{max}(AX_k)} \leq \frac{\lambda_{min}(AX_{k+1})}{\lambda_{max}(AX_{k+1})}
\end{align}
This means that our proposed rate upper bound is non-increasing. The result in \eqref{eq:decrkappa} is simply because from Thm.\ref{thm:interlacing} one can see that the largest eigenvalue of $AX_k$ can only decrease while the smallest eigenvalue can only increase. Using \eqref{eq:convsamplingmax}, we see from Thm. \ref{thm:upperbadamax} that our upper bound can only decrease at each iteration. Note that such concise conclusion is rather difficult if we work with \eqref{eq:convsampingtr}. Indeed, in such scenario the interlacing theorem does not yield an immediate result with respect to the monotonicity for the rate upper bound. This is because the first upper bound \eqref{eq:upperbtrada} (obtained in \cite{Gower1}) depends on the trace of $AX_k$ on the denominator. Then, we do not have a guarantee on the monotonicity of such trace (at least not from Thm. \ref{thm:interlacing} because the spectrum of $AX_k$ has eigenvalues both below and above $1$) and we cannot conclude something about the monotonicity of the bound in \eqref{eq:upperbtrada}.
\vspace{-3mm}
\begin{figure}[H]
  \centering
  \includegraphics[height=0.7\columnwidth,width=1.1\columnwidth]{ratemax.eps}
  
  \vspace{-2mm}
  \caption{$1-\rho_k=\lambda_{\text{min}}(A^{-1/2}\E\big[Z|X_k\big]A^{-1/2})$ and lower/upper bounds (as in equations \eqref{eq:lowerbada} and \eqref{eq:upperbmaxada} respectively). These plots correspond to the column variant of adaRBFGS with $p_i$ as in \eqref{eq:convsamplingmax}. \label{fig:ratemax}}  
  
\end{figure}
A non-increasing rate upper bound (as the one we propose) has an immediate consequences (as pointed out in \cite{Stich1}). First, the adaptive version of RBFGS (e.g. adaRBFGS) can only be better (in expectation) than the non-adaptive version. 
\begin{customcorollary}{1}[AdaRBFGS can only be better, in expectation, than RBFGS]
If $\lambda_{min}(A) \leq 1$ then $\prod_{i=0}^{k-1}\rho\prime_{i} \leq \rho\prime^{k}_0$. \\
Where at iterate $k$ we have $\rho_k\prime = \rho_{\text{upper}}=1 - \kappa(AX_k)^{-1}$
\begin{proof}
Set $\rho\prime_0$ as the rate of the non-adaptive RBFGS and the result follows immediately from inequality \eqref{eq:decrkappa} above.
\end{proof}
\end{customcorollary}
\subsection{Convergence}\label{sect:convergence}
As we saw in our experiments (and conjectured in \cite{Gower1}) the eigenvalues might converge to a distribution centered at 1 and with very low variance.\\
We have found, in fact, that a good framework for studying such behavior is given by some well-known results from Random Matrix theory (which we borrow from \cite{Tao}).\\
In order to explain how can adaRBFGS make progress at each iteration we refer to the \textit{eigenvalue repulsion} property (Exercise 1.3.15 in \cite{Tao}). This is, actually, a direct consequence of the interlacing spectrum theorem. In addition and it reveals a fundamental relation between eigenvalues and how these change as we iterate. The property we present (eigenvalue repulsion) is in fact used to prove some of the main results in \cite{Tao} (see for example the Dyson Brownian motion characterization of the spectrum). We can easily see such property in the case of a rank-1 update. 
\begin{customcorollary}{2}[Eigenvalue repulsion, Tao 2011]
Let $\lambda$ be an eigenvalue for $AX_{k}$ that is different from all eigenvalues for $AX_{k+1}$, and w.l.o.g assume $AX_{k+1}$ is a minor of $AX_k$ obtained by removing the rightmost column and last row of $AX_k$. We can then write:
\begin{align}\label{eq:eigvalequation}
	(AX_{k})_{n,n}-\lambda = \sum_{j=1}^{n-1}\frac{(u_j(AX_{k+1})^TR)^2}{\lambda_j(AX_{k+1})-\lambda}
\end{align}
Where $u_j$ is the $j^{th}$ eigenvector and $X$ is a vector we cut from $AX_k$ to get to $AX_{k+1}$ as its minor (equivalent definition for compression in Section 4.3 of \cite{Horn}).

\begin{proof}
We have $AX_ku=\lambda u$ where $u^T=[u_t^T ,u_b]$ ($u$ being $\lambda$'s eigenvector), $u_t \in \R^{n-1\times 1}$ and $u_b \in \R$.
 By expanding the matrix multiplications in blocks we have $AX_{k+1}u_t + Ru_b = \lambda u_t \Rightarrow$ 
\begin{align}\label{eq:proofrepulsion}
	(AX_{k+1}-\lambda I)u_t= -u_bR
\end{align}

Because we maintain symmetric and positive definite iterates $X_k$ (c.f. \eqref{eq:updaterule}) we know $A^{1/2}X_{k+1}A^{1/2}$ is symmetric and with positive eigenvalues. Hence, we can write $(AX_{k+1}-\lambda I)^{-1}=\sum_j^{n-1} \frac{1}{\lambda_j(AX_{k+1}-\lambda} u_j(AX_{k+1})u_j(AX_{k+1})^T$ (e.g. its spectral decomposition).\\
Then, from the last equality in the system $AX_ku=\lambda u$ we get $u_b((AX_k)_{n,n}-\lambda)=-R^Tu_t$. From \eqref{eq:proofrepulsion} we have $(AX_k)_{n,n}-\lambda=R^T(AX_{k+1}-\lambda I)^{-1}R$ and the result follows.
\end{proof}
\end{customcorollary}

Note that the result above holds without lost of generality because in the case when $AX_{k+1}$ is a minor that is obtained by removing multiple columns from $AX_k$ then $R$ is just a column block matrix and we just have a larger system of equations.
See from the above formula that $\lambda$ is a rational function with removable poles at $\lambda_j(AX_{k+1})$. This means that as long as the update direction from $AX_k$ to $AX_{k+1}$ does not lead to an orthogonal eigenvector to $R$ then eigenvalues repel each other between iterations.\\
It turns out that there is a set of results in Random Matrix theory that characterize the distribution of eigenvalues and these stem from an interlacing spectrum. For this purpose, we work with the Empirical Spectrum Distribution (ESD). Namely,
\begin{customdefinition}{1}[Empirical spectral distribution]
\begin{align}\label{eq:esd}
	\mu_{AX_k} = \frac{1}{M_k} \sum _{j=1}^{M_k} \delta_{\frac{1}{\sqrt{M_k}}\lambda_j(AX_k)}
\end{align}

Where $M_k$ be the multiplicity of the orthogonal eigenspace (e.g. $n$ minus multiplicity of $1$ as eigenvalue of $AX_k$) to the one corresponding to $1$ for $AX_k$. 
\end{customdefinition}
We scale the eigenvalues in \eqref{eq:esd} by the inverse square root to properly scale the variance as it is typical in estimators that consist of sums (details in \cite{Tao}).
Now, the reason why we use $M_k$ is that matrices resulting from compression (e.g. equation \eqref{eq:compression} which yields Thm.\ref{thm:interlacing}) can be alternatively characterized as minors of the matrix that is compressed (relation show in Section 4.3 of \cite{Horn}). Such characterization is interesting because of the results that we can adapt from \cite{Tao} to our setting (see Section \ref{sect:results}). From now on, we can think of the sequence $AX_k$ as successive minors where the number of columns we eliminate to go from one to the other (and rows accordingly) is $M_k-M_{k+1} \leq q$ (where such bound is obtained in Proposition 2.8 from \cite{Gratton}).
The main results that concern us from \cite{Tao} are those that involve the stability of the ESD and its asymptotic convergence properties.\\
A first result is the \textit{Stability of ESD laws with respect to small rank perturbation} for symmetric matrices (note that the objective in \eqref{eq:sketchandproject} acts as a regularizer, thus ensuring that the difference between iterates is a low rank update). What this result says when applied to our setting is the following.

\begin{customconjecture}{1}[Strong convergence of the ESD for the spectrum of AdaRBFGS iterates]\label{conj:convergence}
	$\mu_{AX_{k+1}} \overset{a.s.}{\to} \mu$ for fixed $\mu$.
\end{customconjecture}
 
We make this conjecture because of stronger results related to \ref{thm:stabilityesd} appear in \cite{Tao} as direct consequences of an interlacing spectrum (e.g. Thm. \ref{thm:interlacing}). For example, it can be shown that to establish the circular law (analogous of Central Limit theorem for random matrices) it suffices to use a similar result to \ref{thm:stabilityesd} (c.f. exercise 2.4.2 in \cite{Tao}). Hence, it is plausible that the ESD for $AX_k$ converges to some fixed distribution.\\
However, we do not have a explicit expression for $\mu$ in our setting because at the moment this theory is beyond the scope of this project. As an illustration we mention a generalization of the above result which is the Semicircular law (for sequences of Wigner matrices) which gives an explicit formula for $\mu$. While Wigner matrices are more of a theoretical construction (infinite matrices), there are some results in \cite{Tao} that make the connexion with finite i.i.d and non i.i.d matrices.\\
A second set of results from \cite{Tao} that are relevant to convergence of adaRBFGS are the (weak-strong) versions of the Bai-Yin theorem (c.f. Theorem 2.3.23 in \cite{Tao}). What is important, for us, about Bai-Yin theorems (applied to real symmetric matrix sequences) is that these show that the spectral radius ($\lambda_{max}$ in our setting) of $AX_k$ is asymptotically bounded by a constant (asymptotically almost surely). Then, the convergence of the ESD and Bai-Yin theorems altogether would imply that $AX_k$ is well conditioned asymptotically almost surely.
\vspace{-3mm}
\begin{figure}[H]
  \centering
  \includegraphics[height=0.7\columnwidth,width=1.1\columnwidth]{ratedim.eps}   \vspace{-2mm}
  \caption{Rates for adaRBFGS (column variant) as a function of the dimension of the sketch (e.g. q). \label{fig:ratedim}}  
\end{figure}
\subsection{Dependence on sketch dimension}\label{sect:sketchdim}
Our last result concerns a particular behavior we have observed during the experiments which is the dependence of the number of iterations on the sketch dimension $q$. A first result in this direction is the lower bound presented in \cite{Gower1} for the non-adaptive RBFGS (constant rate), which is 
\begin{align} \label{eq:lowerbddimgower}
	1-\frac{\E[q]}{n}\leq \rho 
\end{align}
What inequality \eqref{eq:lowerbddimgower} implies is that increasing the sketch dimension $q$ can only make the constant rate smaller. Nonetheless, there is a clear trade-off with respect to the complexity per iteration (floating point operations) which is dominated by a matrix inversion (see update rule in Section \ref{sect:rbfgs}) of order $q$ e.g. $O(q^{3})$. Now, we want to know how the sketch dimension depends on the rate for the adaptive setting (e.g. adaRBFGS). From numerical experiments as in Fig. \ref{fig:ratedim} we see that the $1-\rho_k$ can only converge faster as we increase $q$. One can justify this observation with a result stemming (again) from Thm. \ref{thm:interlacing} which is a characterization of the stability of the ESD (which comes from 2.4.1 in \cite{Tao}).

\begin{customcorollary}{3}[Stability of ESD]\label{thm:stabilityesd}
Let $M_k$ and $\mu_{AX_k}$ as defined in \eqref{eq:esd}. We then have:
\begin{align}\label{eq:stabilityofESD}
	\frac{M_k}{M_{k+1}}&\mu_{AX_k}([-\infty,\lambda])-\frac{q}{M_{k+1}}\\ \nonumber
    &\leq \mu_{AX_{k+1}}([-		 \infty,\lambda])\\ \nonumber
    &\leq \frac{M_{k}}{M_{k+1}}\mu_{AX_k}([-\infty,\lambda]) \nonumber
\end{align}
with $0<c<1$.
\begin{proof}
First we prove the second inequality. By Thm. \ref{thm:interlacing}, we have that for each $i \in [n-q]$ we have $\lambda_i(AX_{k+1}) \leq \lambda_{i+q}(AX_k)$. Then, $\frac{M_{k+1}}{M_k} \frac{1}{\sqrt{M_{k+1}}} \lambda_i(AX_{k+1}) \leq \frac{1}{\sqrt{M_{k}}} \lambda_{i+q}(AX_k)$ as $M_k\geq M_{k+1}$ by Proposition 2.8 in \cite{Gratton}. And the inequality follows (point-wise) from the definition of ESD in \eqref{eq:esd}.\\
For the first inequality we proceed as for the second inequality but using $\lambda_i(AX_k) \leq \lambda_i(AX_{k+1}) $ for $i \in [n-q]$. Now, note that because $M_k\geq M_{k+1}$ the contribution of those additional eigenvalues (e.g. difference in cardinality between the orthogonal eigenspace to $1$ for $AX_k$ and $AX_{k+1}$ respectively) can make $\mu_{AX_k}[-\infty,\lambda]\geq \mu_{AX_{k+1}}[-\infty,\lambda]$. However, we can bound the amount of such additional eigenvalues by Proposition 2.8 of \cite{Gratton} as follows: $M_k - M_{k+1} \leq q$. Then, the contribution of these additional eigenvalues to $\mu_{AX_k}$ is bounded by $\frac{q}{M_{k+1}}\geq \frac{q}{M_k}$ and the result follows.
\end{proof}
\end{customcorollary}

Observe from this last result that when $q$ is large then the ESD can change quite a lot from one iteration to the other. However, for $q$ very small the ESD is stable (e.g. changes very little) in one iteration. In this case having an unstable ESD (large $q$) can be beneficial. For example, say we have large $\lambda_{max}(A)$ meaning that we want to eliminate, as quickly as possible, the tail (which implies a large condition number for $AX_k$) composed of large eigenvalues. In the latter scenario, a stable transition in eigenvalue distribution could take quite long to eliminate a long tail (as we can confirm in Fig. \ref{fig:ratedim}) as we make little progress per iteration. A part from Corollary \ref{thm:stabilityesd}, the factorization in \eqref{eq:compression} offers some complementary intuition on the observed experimental results. That is, we see in \eqref{eq:compression} that the number of eigenvalues to compress is $n-q$ (e.g. can only be smaller for larger $q$). These eigenvalues seem to converge, nonetheless, to a distribution centered at $1$ (c.f. \ref{sect:convergence}) independent of the dimension of the sketch $q$. Hence, to preserve the variance (same distribution in all cases) of such limiting distribution, less eigenvalues (e.g. large $q$) should be more spread compared to the situation when we have more eigenvalues to compress (e.g. smaller $q$). Now, the jumps that eigenvalues can make from iteration $k$ to $k+1$ is bounded by the distances between eigenvalues for $AX_k$ from Thm. \ref{thm:interlacing}. Hence, having larger distances between eigenvalues in $AX_k$ and an unstable (c.f. \ref{thm:stabilityesd}) transitions can eliminate those unwanted long eigenvalue tails quicker.\\
Lastly, we mention that the latter remarks do not imply that the number of iterations monotonically decreases with $q$. This is because we base the argument on bounds for the eigenvalue jumps, and change in distribution. Moreover, the convergence of the ESD to a fixed distribution is still an hypothesis (e.g. Conjecture \ref{conj:convergence}). Hence, a proof of Conjecture \ref{conj:convergence} is a possible direction for future research as well as the possibility of monotonic behavior in the number of iterations with respect to $q$.
\section{Conclusion}\label{sect:conclusion}
In this project we have analyzed the rate for adaRBFGS which is a new promising method that can be used as a primitive for first and second order methods. In the process we have surveyed related literature that serves as basis for adaRBFGS and pointed out some additional results that are useful for a finer grained analysis of the algorithm's rate (which were apparently unnoticed by the authors in \cite{Gower1}). A first such result and the starting point of our analysis is the interlacing spectrum theorem (see Subsection \ref{sect:interlacing}). Such property suggests that the spectrum of $AX_k$ shrinks as we iterate, and that eigenvalues accumulate at 1 (which was conjectured in \cite{Gower1}). In addition, we propose a different convenient sampling strategy (to sample sketch matrices for adaRBFGS) for which we get (c.f. \ref{sect:convsampling}) a simpler theoretical analysis for the rate (using the interlacing theorem). We observed as consequence of this proposed sampling strategy that both a lower and upper bound we derived look, experimentally, much tighter than previously stated bounds in \cite{Gower1}. Then, as direct consequence of the form of our upper bound and the interlacing theorem we saw that the rate upper bound is non-increasing and that adaRBFGS outperforms RBFGS (non adaptive version). We point out, however, that we observed experimentally that the true rate $\rho_k$ is also non-increasing. However, the proof of such result is not as straightforward. In fact, if this result is of interest for future result we suggest looking at the relation between subsequent column matrices to be sampled from $L_k$ and $L_{k+1}$, namely $S_i^k$ and $S_i^{k+1}$. It is not hard to see that (from the Courant-Fisher characterization of eigenvalues) that if one could prove either $S_i^k \succ S_i^{k+1}$ (or the opposite) at each iteration, then $\rho_k$ is non-increasing. Moreover, $\rho_k$ for adaRBFGS seems to have better convergence behavior in our experiments. With these results in mind, we embarked on the analysis of convergence for the rate. For this purpose we borrowed results from random matrix theory. This is because such theory has a rich description on the asymptotic and stochastic properties for the spectrum of sequence of matrices such as those we generate for adaRBFGS. Namely, these results describe stability and convergence modes for the Empirical Spectrum Distribution (ESD), that gives us additional theoretical justification on the experimental results (centered distributions around 1) we see. While the results we used are rather general, there is potentially room for specialization as the ones existing for other type of matrices (such as the semi-circle law in \cite{Tao}). Lastly, we used the stability of the ESD to investigate the relation between the sketch dimension $q$ and the true adaRBFGS rate. While there is a result for the non-adaptive version (Theorem 6.1 in \cite{Gower1}) there was no obvious way to extend this to adaRBFGS which is why we investigated this behavior. A next step would be to make explicit the trade-off between the number of iterations and flops per iteration in terms of $q$ and $n$ (number of dimensions). Such question is appealing as it would answer a very practical question for the use of adaRBFGS: how to choose $q$ that is most efficient for my problem?. Lastly, we mention an interesting direction for future research. This consists of going beyond proving that the rate (or its upper bound) are non-increasing (as proved in Subsection \ref{sect:convsampling}) and finding the sequence of decrement ratios between subsequent rates. Such a result is plausible as the convergence time depends mainly on $\lambda_{max}(AX_k)$ and a set of results for bounding the spectral radius have been derived using related to the interlacing spectrum (c.f. \cite{Mercer,Horne}).



\bibliographystyle{IEEEtran}
%\bibliography{literature}
\begin{thebibliography}{1}

\bibitem{Gower1}
\textit{Randomized quasi-Newton updates are linearly convergent matrix inversion algorithms.}
Robert M. Gower and Peter Richt\'arik. arXiv:1602.01768, 2016.

\bibitem{Gower2}
\textit{Randomized iterative methods for linear systems.} Robert M. Gower and Peter Richt\'arik. SIAM Journal on Matrix Analysis and Applications 36.4, pp. 1660-1690, 2015.
\bibitem{Gower3}
\textit{Stochastic Block BFGS: Squeezing More Curvature our of Data.} R.M. Gower, D. Goldfarb and P. Richt\'arik. Proceedings of The 33rd International Conference on Machine Learning, PMLR 48:1869-1878, 2016.
\bibitem{Gower4}
\textit{Linearly convergent randomized iterative methods for computing the pseudoinverse.} Robert M. Gower and Peter Richtarik. Preprint, December 2016.

\bibitem{Gowercode}
\textit{A suite of randomized methods for inverting positive definite matrices implemented in MATLAB.} Robert M Gower. \url{http://www.maths.ed.ac.uk/~prichtar/i_software.html}.

\bibitem{Gratton}
\textit{On a class of limited memory preconditioners for large-scale nonlinear least-squares problems.} SIAM Journal on Optimization 21.3, pp. 912-935, 2011.

\bibitem{Horn}
\textit{Matrix Analysis.} R.A. Horn and C.R. Johnson. Cambridge University Press, Cambridge, England, 1999.
\bibitem{Nazareth}
\textit{A relationship between the BFGS and conjugate gradient algorithms and its implications for new algorithms.} L. Nazareth. SIAM Journal on Numerical Analysis, 16:794-800, 1979.

\bibitem{Fletcher}
\textit{A new approach to variable metric algorithms.} R. Fletcher. Computer Journal, 13:317-322, 1970.

\bibitem{Tao}
\textit{Topics in Random Matrix Theory.} Terrence Tao. Graduate Studies in Mathematics, Volume 132. American Mathematical Society, 2012.

\bibitem{Fang}
\textit{Eigenvalue Inequalities for Matrix Product.} Fuzhen Zhang and Qingling Zhang. IEEE Transactions on Automatic Control, 2006.

\bibitem{J Tao}
\textit{The Cauchy interlacing theorem in simple Euclidean Jordan algebras and some consequences.}
M. Seetharama Gowda and J. Tao. Linear and Multilinear Algebra, 16:41, 2009.

\bibitem{Mercer}
\textit{Cauchy's interlace theorem and lower bounds for the spectral radius.} A.Mcd. Mercer and Peter R. Mercer. International Journal of mathematics and Mathematical Sciences. Vol. 23, Issue 8, pp. 563-566, 2000.

\bibitem{Horne}
\textit{Lower bounds for the spectral radius of a matrix.} Bill G. Horne. NECI Technical Report 95-14, NEC Research Institute, 1995.

\bibitem{Stich1}
Sebastian U. Stich, Robert M. Gower and Peter Richt\'arik.
\textit{Complexity of Adaptive Randomized BFGS for Matrix Inversion.}
Preprint, 2016.

\bibitem{Stich2}
\textit{Variable metric random pursuit.} S. U. Stich, C.L. Muller, and B.Gartner. Mathematical
Programming 156.1 (2015), pp. 549-579.

\bibitem{Leventhal}
\textit{Randomized Hessian estimation and directional search.} D.Leventhal and A.Lewis. Optimization 60.3(2011), pp. 329-345.

\bibitem{variancereduct}
\textit{Accelerating stochastic gradient descent using predictive variance reduction.}Rie Johnson, Tong Zhang. Advances in Neural Information Processing Systems. pp. 315-323, 2013.

\end{thebibliography}

\end{document}
